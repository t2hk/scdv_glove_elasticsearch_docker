{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. データをクローリングする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p html excel csv json tokenized vector log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 事故事例(Excel系)と、事例・状況・原因・対策（HTML系）の2種類を取得する。\n",
    "! ./get_doc.sh > ./log/get_doc.log 2>&1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 取得したデータをCSVまたはJSONに変換する。\n",
    "* 事故事例のExcelをCSVに変換する。\n",
    "* 事例・状況・原因・対策のHTMLはJSONに変換する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excel to CSV\n",
    "!python excel_to_csv.py ./excel/ ./csv/ > ./log/excel_to_csv.log 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML to JSON\n",
    "!python html_to_json.py ./html/ ./json/ > ./log/html_to_json.log 2>&1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ElasticsearchにIndexを登録する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"acknowledged\":true,\"shards_acknowledged\":true,\"index\":\"anzen\"}"
     ]
    }
   ],
   "source": [
    "# 事例・状況・原因・対策用のインデックスを登録\n",
    "! curl -XPUT -H \"Content-Type: application/json\" elasticsearch:9200/anzen -d @es_anzen_schema.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"acknowledged\":true,\"shards_acknowledged\":true,\"index\":\"accident\"}"
     ]
    }
   ],
   "source": [
    "# 事例用のインデックスを登録\n",
    "! curl -XPUT -H \"Content-Type: application/json\" elasticsearch:9200/accident -d @es_accident_schema.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Elasticsearchにデータをロードする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 事例・状況・原因・対策データをロード(anzenインデックス)\n",
    "! ./load_anzen_bulk_es.sh > ./log/load_anzen_bulk_es.log 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 事例用データをロード(accidentインデックス)\n",
    "!python load_accident_es.py --host elasticsearch --index accident --input_dir ./csv/  > ./log/load_accident_es.log 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Elasticsearchでトークナイズし、結果をCSV形式とテキスト形式で保存する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 事例・状況・原因・対策データの文章をトークナイズし、CSVとTXTファイルとして保存する\n",
    "!python es_anzen_tokenize.py --host elasticsearch --index anzen --output tokenized/anzen_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 事例用データの文章をトークナイズし、CSVとTXTファイルとして保存する\n",
    "!python es_accident_tokenize.py --host elasticsearch --index accident --output tokenized/accident_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. GloVeで単語を学習する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'GloVe' already exists and is not an empty directory.\n",
      "mkdir -p build\n",
      "gcc -c src/vocab_count.c -o build/vocab_count.o -lm -pthread -O3 -march=native -funroll-loops -Wall -Wextra -Wpedantic\n",
      "gcc -c src/cooccur.c -o build/cooccur.o -lm -pthread -O3 -march=native -funroll-loops -Wall -Wextra -Wpedantic\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kmerge_files\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:180:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
      "         \u001b[01;35m\u001b[Kfread(&new, sizeof(CREC), 1, fid[i])\u001b[m\u001b[K;\n",
      "         \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:190:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
      "     \u001b[01;35m\u001b[Kfread(&new, sizeof(CREC), 1, fid[i])\u001b[m\u001b[K;\n",
      "     \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:203:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
      "         \u001b[01;35m\u001b[Kfread(&new, sizeof(CREC), 1, fid[i])\u001b[m\u001b[K;\n",
      "         \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "gcc -c src/shuffle.c -o build/shuffle.o -lm -pthread -O3 -march=native -funroll-loops -Wall -Wextra -Wpedantic\n",
      "\u001b[01m\u001b[Ksrc/shuffle.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kshuffle_merge\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Ksrc/shuffle.c:96:17:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
      "                 \u001b[01;35m\u001b[Kfread(&array[i], sizeof(CREC), 1, fid[j])\u001b[m\u001b[K;\n",
      "                 \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/shuffle.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kshuffle_by_chunks\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Ksrc/shuffle.c:161:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
      "         \u001b[01;35m\u001b[Kfread(&array[i], sizeof(CREC), 1, fin)\u001b[m\u001b[K;\n",
      "         \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "gcc -c src/glove.c -o build/glove.o -lm -pthread -O3 -march=native -funroll-loops -Wall -Wextra -Wpedantic\n",
      "\u001b[01m\u001b[Ksrc/glove.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kinitialize_parameters\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Ksrc/glove.c:94:17:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
      "                 \u001b[01;35m\u001b[Kfread(&W[a], sizeof(real), 1, fin)\u001b[m\u001b[K;\n",
      "                 \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kglove_thread\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Ksrc/glove.c:146:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Kfread\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
      "         \u001b[01;35m\u001b[Kfread(&cr, sizeof(CREC), 1, fin)\u001b[m\u001b[K;\n",
      "         \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
      "gcc -c src/common.c -o build/common.o -lm -pthread -O3 -march=native -funroll-loops -Wall -Wextra -Wpedantic\n",
      "gcc build/vocab_count.o build/common.o -o build/vocab_count -lm -pthread -O3 -march=native -funroll-loops -Wall -Wextra -Wpedantic\n",
      "gcc build/cooccur.o build/common.o -o build/cooccur -lm -pthread -O3 -march=native -funroll-loops -Wall -Wextra -Wpedantic\n",
      "gcc build/shuffle.o build/common.o -o build/shuffle -lm -pthread -O3 -march=native -funroll-loops -Wall -Wextra -Wpedantic\n",
      "gcc build/glove.o build/common.o -o build/glove -lm -pthread -O3 -march=native -funroll-loops -Wall -Wextra -Wpedantic\n"
     ]
    }
   ],
   "source": [
    "# GloVeを取得する。\n",
    "! git clone https://github.com/stanfordnlp/GloVe.git\n",
    "! cd GloVe && make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トークナイズしたanzenとaccidentのテキストファイルを結合する。\n",
    "! cat tokenized/anzen_tokenized.txt tokenized/accident_tokenized.txt > tokenized/all_tokens.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "$ GloVe/build/vocab_count -min-count 0 -verbose 2 < tokenized/all_tokens.txt > ./vector/glove_vocab.txt\n",
      "BUILDING VOCABULARY\n",
      "Processed 0 tokens.\u001b[11G100000 tokens.\u001b[11G200000 tokens.\u001b[11G300000 tokens.\u001b[11G400000 tokens.\u001b[11G500000 tokens.\u001b[11G600000 tokens.\u001b[11G700000 tokens.\u001b[11G800000 tokens.\u001b[11G900000 tokens.\u001b[11G1000000 tokens.\u001b[11G1100000 tokens.\u001b[11G1200000 tokens.\u001b[11G1300000 tokens.\u001b[11G1400000 tokens.\u001b[11G1500000 tokens.\u001b[11G1600000 tokens.\u001b[11G1700000 tokens.\u001b[11G1800000 tokens.\u001b[11G1900000 tokens.\u001b[11G2000000 tokens.\u001b[11G2100000 tokens.\u001b[11G2200000 tokens.\u001b[11G2300000 tokens.\u001b[11G2400000 tokens.\u001b[11G2500000 tokens.\u001b[11G2600000 tokens.\u001b[11G2700000 tokens.\u001b[11G2800000 tokens.\u001b[11G2900000 tokens.\u001b[11G3000000 tokens.\u001b[11G3100000 tokens.\u001b[11G3200000 tokens.\u001b[0GProcessed 3208826 tokens.\n",
      "Counted 27931 unique words.\n",
      "Using vocabulary of size 27931.\n",
      "\n",
      "$ GloVe/build/cooccur -memory 4.0 -vocab-file ./vector/glove_vocab.txt -verbose 2 -window-size 15 < tokenized/all_tokens.txt > ./vector/glove_cooccurrence.bin\n",
      "COUNTING COOCCURRENCES\n",
      "window size: 15\n",
      "context: symmetric\n",
      "max product: 13752509\n",
      "overflow length: 38028356\n",
      "Reading vocab from file \"./vector/glove_vocab.txt\"...loaded 27931 words.\n",
      "Building lookup table...table contains 69261183 elements.\n",
      "Processing token: 0\u001b[19G100000\u001b[19G200000\u001b[19G300000\u001b[19G400000\u001b[19G500000\u001b[19G600000\u001b[19G700000\u001b[19G800000\u001b[19G900000\u001b[19G1000000\u001b[19G1100000\u001b[19G1200000\u001b[19G1300000\u001b[19G1400000\u001b[19G1500000\u001b[19G1600000\u001b[19G1700000\u001b[19G1800000\u001b[19G1900000\u001b[19G2000000\u001b[19G2100000\u001b[19G2200000\u001b[19G2300000\u001b[19G2400000\u001b[19G2500000\u001b[19G2600000\u001b[19G2700000\u001b[19G2800000\u001b[19G2900000\u001b[19G3000000\u001b[19G3100000\u001b[19G3200000\u001b[0GProcessed 3208826 tokens.\n",
      "Writing cooccurrences to disk........2 files in total.\n",
      "Merging cooccurrence files: processed 0 lines.\u001b[39G100000 lines.\u001b[39G200000 lines.\u001b[39G300000 lines.\u001b[39G400000 lines.\u001b[39G500000 lines.\u001b[39G600000 lines.\u001b[39G700000 lines.\u001b[39G800000 lines.\u001b[39G900000 lines.\u001b[39G1000000 lines.\u001b[39G1100000 lines.\u001b[39G1200000 lines.\u001b[39G1300000 lines.\u001b[39G1400000 lines.\u001b[39G1500000 lines.\u001b[39G1600000 lines.\u001b[39G1700000 lines.\u001b[39G1800000 lines.\u001b[39G1900000 lines.\u001b[39G2000000 lines.\u001b[39G2100000 lines.\u001b[39G2200000 lines.\u001b[39G2300000 lines.\u001b[39G2400000 lines.\u001b[39G2500000 lines.\u001b[39G2600000 lines.\u001b[39G2700000 lines.\u001b[39G2800000 lines.\u001b[39G2900000 lines.\u001b[39G3000000 lines.\u001b[39G3100000 lines.\u001b[39G3200000 lines.\u001b[39G3300000 lines.\u001b[39G3400000 lines.\u001b[39G3500000 lines.\u001b[39G3600000 lines.\u001b[39G3700000 lines.\u001b[39G3800000 lines.\u001b[39G3900000 lines.\u001b[39G4000000 lines.\u001b[39G4100000 lines.\u001b[39G4200000 lines.\u001b[39G4300000 lines.\u001b[39G4400000 lines.\u001b[39G4500000 lines.\u001b[39G4600000 lines.\u001b[39G4700000 lines.\u001b[39G4800000 lines.\u001b[39G4900000 lines.\u001b[39G5000000 lines.\u001b[39G5100000 lines.\u001b[39G5200000 lines.\u001b[39G5300000 lines.\u001b[39G5400000 lines.\u001b[39G5500000 lines.\u001b[39G5600000 lines.\u001b[39G5700000 lines.\u001b[39G5800000 lines.\u001b[39G5900000 lines.\u001b[39G6000000 lines.\u001b[39G6100000 lines.\u001b[39G6200000 lines.\u001b[39G6300000 lines.\u001b[39G6400000 lines.\u001b[39G6500000 lines.\u001b[39G6600000 lines.\u001b[39G6700000 lines.\u001b[39G6800000 lines.\u001b[39G6900000 lines.\u001b[39G7000000 lines.\u001b[39G7100000 lines.\u001b[39G7200000 lines.\u001b[39G7300000 lines.\u001b[39G7400000 lines.\u001b[0GMerging cooccurrence files: processed 7445714 lines.\n",
      "\n",
      "$ GloVe/build/shuffle -memory 4.0 -verbose 2 < ./vector/glove_cooccurrence.bin > ./vector/glove_cooccurrence_shuf.bin\n",
      "Using random seed 1581422653\n",
      "SHUFFLING COOCCURRENCES\n",
      "array size: 255013683\n",
      "Shuffling by chunks: processed 0 lines.\u001b[22Gprocessed 7445714 lines.\n",
      "Wrote 1 temporary file(s).\n",
      "Merging temp files: processed 0 lines.\u001b[31G7445714 lines.\u001b[0GMerging temp files: processed 7445714 lines.\n",
      "\n",
      "$ GloVe/build/glove -save-file ./vector/glove_vectors -threads 8 -input-file ./vector/glove_cooccurrence_shuf.bin -x-max 10 -iter 50 -vector-size 50 -binary 2 -vocab-file ./vector/glove_vocab.txt -verbose 2\n",
      "TRAINING MODEL\n",
      "Read 7445714 lines.\n",
      "Initializing parameters...Using random seed 1581422656\n",
      "done.\n",
      "vector size: 50\n",
      "vocab size: 27931\n",
      "x_max: 10.000000\n",
      "alpha: 0.750000\n",
      "02/11/20 - 09:04.17PM, iter: 001, cost: 0.138573\n",
      "02/11/20 - 09:04.18PM, iter: 002, cost: 0.095891\n",
      "02/11/20 - 09:04.20PM, iter: 003, cost: 0.079914\n",
      "02/11/20 - 09:04.21PM, iter: 004, cost: 0.069852\n",
      "02/11/20 - 09:04.22PM, iter: 005, cost: 0.063224\n",
      "02/11/20 - 09:04.24PM, iter: 006, cost: 0.058910\n",
      "02/11/20 - 09:04.25PM, iter: 007, cost: 0.055971\n",
      "02/11/20 - 09:04.26PM, iter: 008, cost: 0.053905\n",
      "02/11/20 - 09:04.28PM, iter: 009, cost: 0.052325\n",
      "02/11/20 - 09:04.29PM, iter: 010, cost: 0.051123\n",
      "02/11/20 - 09:04.31PM, iter: 011, cost: 0.050200\n",
      "02/11/20 - 09:04.32PM, iter: 012, cost: 0.049370\n",
      "02/11/20 - 09:04.33PM, iter: 013, cost: 0.048714\n",
      "02/11/20 - 09:04.35PM, iter: 014, cost: 0.048143\n",
      "02/11/20 - 09:04.36PM, iter: 015, cost: 0.047639\n",
      "02/11/20 - 09:04.37PM, iter: 016, cost: 0.047229\n",
      "02/11/20 - 09:04.39PM, iter: 017, cost: 0.046790\n",
      "02/11/20 - 09:04.40PM, iter: 018, cost: 0.046471\n",
      "02/11/20 - 09:04.41PM, iter: 019, cost: 0.046147\n",
      "02/11/20 - 09:04.43PM, iter: 020, cost: 0.045919\n",
      "02/11/20 - 09:04.44PM, iter: 021, cost: 0.045634\n",
      "02/11/20 - 09:04.45PM, iter: 022, cost: 0.045408\n",
      "02/11/20 - 09:04.47PM, iter: 023, cost: 0.045178\n",
      "02/11/20 - 09:04.48PM, iter: 024, cost: 0.044957\n",
      "02/11/20 - 09:04.49PM, iter: 025, cost: 0.044781\n",
      "02/11/20 - 09:04.51PM, iter: 026, cost: 0.044622\n",
      "02/11/20 - 09:04.52PM, iter: 027, cost: 0.044432\n",
      "02/11/20 - 09:04.53PM, iter: 028, cost: 0.044294\n",
      "02/11/20 - 09:04.55PM, iter: 029, cost: 0.044143\n",
      "02/11/20 - 09:04.56PM, iter: 030, cost: 0.044036\n",
      "02/11/20 - 09:04.58PM, iter: 031, cost: 0.043870\n",
      "02/11/20 - 09:04.59PM, iter: 032, cost: 0.043738\n",
      "02/11/20 - 09:05.00PM, iter: 033, cost: 0.043628\n",
      "02/11/20 - 09:05.02PM, iter: 034, cost: 0.043529\n",
      "02/11/20 - 09:05.03PM, iter: 035, cost: 0.043416\n",
      "02/11/20 - 09:05.04PM, iter: 036, cost: 0.043326\n",
      "02/11/20 - 09:05.06PM, iter: 037, cost: 0.043210\n",
      "02/11/20 - 09:05.07PM, iter: 038, cost: 0.043123\n",
      "02/11/20 - 09:05.08PM, iter: 039, cost: 0.043036\n",
      "02/11/20 - 09:05.10PM, iter: 040, cost: 0.042956\n",
      "02/11/20 - 09:05.11PM, iter: 041, cost: 0.042889\n",
      "02/11/20 - 09:05.13PM, iter: 042, cost: 0.042785\n",
      "02/11/20 - 09:05.14PM, iter: 043, cost: 0.042717\n",
      "02/11/20 - 09:05.15PM, iter: 044, cost: 0.042650\n",
      "02/11/20 - 09:05.17PM, iter: 045, cost: 0.042577\n",
      "02/11/20 - 09:05.18PM, iter: 046, cost: 0.042520\n",
      "02/11/20 - 09:05.19PM, iter: 047, cost: 0.042443\n",
      "02/11/20 - 09:05.21PM, iter: 048, cost: 0.042378\n",
      "02/11/20 - 09:05.22PM, iter: 049, cost: 0.042334\n",
      "02/11/20 - 09:05.23PM, iter: 050, cost: 0.042247\n"
     ]
    }
   ],
   "source": [
    "# GloVeで学習する。\n",
    "! ./glove.sh "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCDVで文章をベクトル化する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anzenとaccidentのCSVをマージする。ベクトル化に必要な列のみ選別し、列名を統一する。\n",
    "!python merge_csv.py --input_anzen tokenized/anzen_tokenized.csv --input_accident tokenized/accident_tokenized.csv --output_csv tokenized/merge_tokenized.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "Clustering Done...\n",
      "Cluster Assignments Saved...\n",
      "Probabilities of Cluster Assignments saved...\n",
      "100%|█████████████████████████████████| 118394/118394 [00:19<00:00, 5962.15it/s]\n",
      "100%|██████████████████████████████████| 13155/13155 [00:00<00:00, 17787.37it/s]\n",
      "train size:118394  vector size:1000\n",
      "test size:13155  vector size:1000\n",
      "Test start...\n",
      "Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cause   0.557692  0.435918  0.489343      1264\n",
      "    measures   0.695862  0.706088  0.700938      1429\n",
      "   situation   0.629565  0.500000  0.557352      1448\n",
      "       title   0.468750  0.297619  0.364078       252\n",
      "      その他の事業   0.581882  0.341513  0.430412       489\n",
      "      保健・衛生業   0.039216  0.400000  0.071429         5\n",
      "        保健衛生   0.043478  0.500000  0.080000         2\n",
      "       保健衛生業   0.181818  0.347826  0.238806        46\n",
      "          商業   0.630972  0.474597  0.541726       807\n",
      "         官公署   0.023256  0.333333  0.043478         3\n",
      "         建設業   0.692008  0.810638  0.746641      3290\n",
      "       接客娯楽業   0.529412  0.394161  0.451883       137\n",
      "      教育・研究業   0.032787  0.285714  0.058824         7\n",
      "       教育研究業   0.176471  0.400000  0.244898        30\n",
      "      映画・演劇業   0.011111  0.142857  0.020619         7\n",
      "      清掃・と畜業   0.614525  0.369128  0.461216       298\n",
      "      畜産・水産業   0.446809  0.543103  0.490272       116\n",
      "         製造業   0.591811  0.595621  0.593710      1553\n",
      "       貨物取扱業   0.360544  0.445378  0.398496       119\n",
      "         農林業   0.695260  0.797927  0.743064       386\n",
      "         通信業   0.254545  0.400000  0.311111        35\n",
      "       運輸交通業   0.619565  0.679650  0.648218      1258\n",
      "      金融・広告業   0.333333  0.578125  0.422857        64\n",
      "          鉱業   0.491228  0.509091  0.500000       110\n",
      "\n",
      "    accuracy                       0.614671     13155\n",
      "   macro avg   0.404246  0.470345  0.400390     13155\n",
      "weighted avg   0.624503  0.614671  0.612961     13155\n",
      "\n",
      "Accuracy:  0.6146712276700874\n",
      "Time taken: 333.27435398101807 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python scdv.py --csv_file ./tokenized/merge_tokenized.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全データについてSCDVベクトルを算出し、Elasticsearchに登録する\n",
    "!python scdv_to_es.py --host elasticsearch --input_csv ./tokenized/merge_tokenized.csv > ./log/scdv_to_es.log 2>&1 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
