{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. データをクローリングする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p html excel csv tokenized vector log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 事故事例(Excel系)と、事例・状況・原因・対策（HTML系）の2種類を取得する。\n",
    "! ./get_doc.sh > ./log/get_doc.log 2>&1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 取得したデータをCSVに変換する。\n",
    "* 事故事例のExcelと、事例・状況・原因・対策のHTMLをcsvに変換する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excel to CSV\n",
    "!python excel_to_csv.py ./excel/ ./csv/ > ./log/excel_to_csv.log 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML to CSV\n",
    "!python html_to_csv.py ./html/ ./csv/anzen_all.csv > ./log/html_to_csv.log 2>&1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ElasticsearchにIndexを登録する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"acknowledged\":true,\"shards_acknowledged\":true,\"index\":\"anzen\"}"
     ]
    }
   ],
   "source": [
    "# 事例・状況・原因・対策用のインデックスを登録\n",
    "! curl -XPUT -H \"Content-Type: application/json\" elasticsearch:9200/anzen -d @es_anzen_csv_schema.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"acknowledged\":true,\"shards_acknowledged\":true,\"index\":\"accident\"}"
     ]
    }
   ],
   "source": [
    "# 事例用のインデックスを登録\n",
    "! curl -XPUT -H \"Content-Type: application/json\" elasticsearch:9200/accident -d @es_accident_schema.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Elasticsearchにデータをロードする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 事例・状況・原因・対策データをロード(anzenインデックス)\n",
    "! python load_anzen_es.py --host elasticsearch --index anzen --input_csv ./csv/anzen_all.csv  > ./log/load_anzen_es.log 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 事例用データをロード(accidentインデックス)\n",
    "!python load_accident_es.py --host elasticsearch --index accident --input_dir ./csv/  > ./log/load_accident_es.log 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Elasticsearchでトークナイズし、結果をCSV形式とテキスト形式で保存する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 事例・状況・原因・対策データの文章をトークナイズし、CSVとTXTファイルとして保存する\n",
    "!python es_anzen_tokenize.py --host elasticsearch --index anzen --output tokenized/anzen_tokenized > ./log/es_anzen_tokenize.log 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 事例用データの文章をトークナイズし、CSVとTXTファイルとして保存する\n",
    "!python es_accident_tokenize.py --host elasticsearch --index accident --output tokenized/accident_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. GloVeで単語を学習する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'GloVe' already exists and is not an empty directory.\n",
      "mkdir -p build\n"
     ]
    }
   ],
   "source": [
    "# GloVeを取得する。最初に1回だけ実行すれば良い。\n",
    "! git clone https://github.com/stanfordnlp/GloVe.git\n",
    "! cd GloVe && make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トークナイズしたanzenとaccidentのテキストファイルを結合する。\n",
    "! cat tokenized/anzen_tokenized.txt tokenized/accident_tokenized.txt > tokenized/all_tokens.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "$ GloVe/build/vocab_count -min-count 0 -verbose 2 < tokenized/all_tokens.txt > ./vector/glove_vocab.txt\n",
      "BUILDING VOCABULARY\n",
      "Processed 0 tokens.\u001b[11G100000 tokens.\u001b[11G200000 tokens.\u001b[11G300000 tokens.\u001b[11G400000 tokens.\u001b[11G500000 tokens.\u001b[11G600000 tokens.\u001b[11G700000 tokens.\u001b[11G800000 tokens.\u001b[11G900000 tokens.\u001b[11G1000000 tokens.\u001b[0GProcessed 1062917 tokens.\n",
      "Counted 22214 unique words.\n",
      "Using vocabulary of size 22214.\n",
      "\n",
      "$ GloVe/build/cooccur -memory 4.0 -vocab-file ./vector/glove_vocab.txt -verbose 2 -window-size 15 < tokenized/all_tokens.txt > ./vector/glove_cooccurrence.bin\n",
      "COUNTING COOCCURRENCES\n",
      "window size: 15\n",
      "context: symmetric\n",
      "max product: 13752509\n",
      "overflow length: 38028356\n",
      "Reading vocab from file \"./vector/glove_vocab.txt\"...loaded 22214 words.\n",
      "Building lookup table...table contains 62967987 elements.\n",
      "Processing token: 0\u001b[19G100000\u001b[19G200000\u001b[19G300000\u001b[19G400000\u001b[19G500000\u001b[19G600000\u001b[19G700000\u001b[19G800000\u001b[19G900000\u001b[19G1000000\u001b[0GProcessed 1062917 tokens.\n",
      "Writing cooccurrences to disk........2 files in total.\n",
      "Merging cooccurrence files: processed 0 lines.\u001b[39G100000 lines.\u001b[39G200000 lines.\u001b[39G300000 lines.\u001b[39G400000 lines.\u001b[39G500000 lines.\u001b[39G600000 lines.\u001b[39G700000 lines.\u001b[39G800000 lines.\u001b[39G900000 lines.\u001b[39G1000000 lines.\u001b[39G1100000 lines.\u001b[39G1200000 lines.\u001b[39G1300000 lines.\u001b[39G1400000 lines.\u001b[39G1500000 lines.\u001b[39G1600000 lines.\u001b[39G1700000 lines.\u001b[39G1800000 lines.\u001b[39G1900000 lines.\u001b[39G2000000 lines.\u001b[39G2100000 lines.\u001b[39G2200000 lines.\u001b[39G2300000 lines.\u001b[39G2400000 lines.\u001b[39G2500000 lines.\u001b[39G2600000 lines.\u001b[39G2700000 lines.\u001b[39G2800000 lines.\u001b[39G2900000 lines.\u001b[39G3000000 lines.\u001b[39G3100000 lines.\u001b[39G3200000 lines.\u001b[39G3300000 lines.\u001b[39G3400000 lines.\u001b[39G3500000 lines.\u001b[39G3600000 lines.\u001b[39G3700000 lines.\u001b[39G3800000 lines.\u001b[39G3900000 lines.\u001b[39G4000000 lines.\u001b[39G4100000 lines.\u001b[39G4200000 lines.\u001b[39G4300000 lines.\u001b[39G4400000 lines.\u001b[39G4500000 lines.\u001b[39G4600000 lines.\u001b[39G4700000 lines.\u001b[39G4800000 lines.\u001b[0GMerging cooccurrence files: processed 4823430 lines.\n",
      "\n",
      "$ GloVe/build/shuffle -memory 4.0 -verbose 2 < ./vector/glove_cooccurrence.bin > ./vector/glove_cooccurrence_shuf.bin\n",
      "Using random seed 1581840537\n",
      "SHUFFLING COOCCURRENCES\n",
      "array size: 255013683\n",
      "Shuffling by chunks: processed 0 lines.\u001b[22Gprocessed 4823430 lines.\n",
      "Wrote 1 temporary file(s).\n",
      "Merging temp files: processed 0 lines.\u001b[31G4823430 lines.\u001b[0GMerging temp files: processed 4823430 lines.\n",
      "\n",
      "$ GloVe/build/glove -save-file ./vector/glove_vectors -threads 8 -input-file ./vector/glove_cooccurrence_shuf.bin -x-max 10 -iter 50 -vector-size 50 -binary 2 -vocab-file ./vector/glove_vocab.txt -verbose 2\n",
      "TRAINING MODEL\n",
      "Read 4823430 lines.\n",
      "Initializing parameters...Using random seed 1581840538\n",
      "done.\n",
      "vector size: 50\n",
      "vocab size: 22214\n",
      "x_max: 10.000000\n",
      "alpha: 0.750000\n",
      "02/16/20 - 05:08.58PM, iter: 001, cost: 0.110711\n",
      "02/16/20 - 05:08.59PM, iter: 002, cost: 0.079980\n",
      "02/16/20 - 05:08.59PM, iter: 003, cost: 0.066841\n",
      "02/16/20 - 05:09.00PM, iter: 004, cost: 0.058057\n",
      "02/16/20 - 05:09.00PM, iter: 005, cost: 0.051476\n",
      "02/16/20 - 05:09.01PM, iter: 006, cost: 0.046541\n",
      "02/16/20 - 05:09.01PM, iter: 007, cost: 0.042950\n",
      "02/16/20 - 05:09.02PM, iter: 008, cost: 0.040369\n",
      "02/16/20 - 05:09.02PM, iter: 009, cost: 0.038456\n",
      "02/16/20 - 05:09.03PM, iter: 010, cost: 0.036986\n",
      "02/16/20 - 05:09.03PM, iter: 011, cost: 0.035839\n",
      "02/16/20 - 05:09.04PM, iter: 012, cost: 0.034897\n",
      "02/16/20 - 05:09.04PM, iter: 013, cost: 0.034129\n",
      "02/16/20 - 05:09.05PM, iter: 014, cost: 0.033477\n",
      "02/16/20 - 05:09.05PM, iter: 015, cost: 0.032942\n",
      "02/16/20 - 05:09.06PM, iter: 016, cost: 0.032456\n",
      "02/16/20 - 05:09.06PM, iter: 017, cost: 0.032030\n",
      "02/16/20 - 05:09.07PM, iter: 018, cost: 0.031667\n",
      "02/16/20 - 05:09.07PM, iter: 019, cost: 0.031333\n",
      "02/16/20 - 05:09.08PM, iter: 020, cost: 0.031037\n",
      "02/16/20 - 05:09.08PM, iter: 021, cost: 0.030776\n",
      "02/16/20 - 05:09.09PM, iter: 022, cost: 0.030535\n",
      "02/16/20 - 05:09.09PM, iter: 023, cost: 0.030307\n",
      "02/16/20 - 05:09.10PM, iter: 024, cost: 0.030099\n",
      "02/16/20 - 05:09.10PM, iter: 025, cost: 0.029915\n",
      "02/16/20 - 05:09.11PM, iter: 026, cost: 0.029740\n",
      "02/16/20 - 05:09.11PM, iter: 027, cost: 0.029568\n",
      "02/16/20 - 05:09.11PM, iter: 028, cost: 0.029420\n",
      "02/16/20 - 05:09.12PM, iter: 029, cost: 0.029275\n",
      "02/16/20 - 05:09.12PM, iter: 030, cost: 0.029149\n",
      "02/16/20 - 05:09.13PM, iter: 031, cost: 0.029017\n",
      "02/16/20 - 05:09.13PM, iter: 032, cost: 0.028899\n",
      "02/16/20 - 05:09.14PM, iter: 033, cost: 0.028789\n",
      "02/16/20 - 05:09.14PM, iter: 034, cost: 0.028686\n",
      "02/16/20 - 05:09.15PM, iter: 035, cost: 0.028584\n",
      "02/16/20 - 05:09.15PM, iter: 036, cost: 0.028485\n",
      "02/16/20 - 05:09.16PM, iter: 037, cost: 0.028395\n",
      "02/16/20 - 05:09.16PM, iter: 038, cost: 0.028311\n",
      "02/16/20 - 05:09.17PM, iter: 039, cost: 0.028228\n",
      "02/16/20 - 05:09.17PM, iter: 040, cost: 0.028151\n",
      "02/16/20 - 05:09.18PM, iter: 041, cost: 0.028075\n",
      "02/16/20 - 05:09.18PM, iter: 042, cost: 0.028005\n",
      "02/16/20 - 05:09.19PM, iter: 043, cost: 0.027939\n",
      "02/16/20 - 05:09.19PM, iter: 044, cost: 0.027865\n",
      "02/16/20 - 05:09.20PM, iter: 045, cost: 0.027824\n",
      "02/16/20 - 05:09.20PM, iter: 046, cost: 0.027752\n",
      "02/16/20 - 05:09.21PM, iter: 047, cost: 0.027683\n",
      "02/16/20 - 05:09.22PM, iter: 048, cost: 0.027618\n",
      "02/16/20 - 05:09.22PM, iter: 049, cost: 0.027551\n",
      "02/16/20 - 05:09.23PM, iter: 050, cost: 0.027503\n"
     ]
    }
   ],
   "source": [
    "# GloVeで学習する。\n",
    "! ./glove.sh "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCDVで文章をベクトル化する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理として学習用のcsvファイルの作成を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anzenとaccidentのCSVをマージする。ベクトル化に必要な列のみ選別し、列名を統一する。\n",
    "!python merge_csv.py --input_anzen tokenized/anzen_tokenized.csv --input_accident tokenized/accident_tokenized.csv --output_csv tokenized/merge_tokenized.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDAトピックモデルを使用して、いくつのトピックに分類すると良いか確認する\n",
    "* 以降のSCDVモデルを構築する際の、GMMのクラスタ数の参考とするため\n",
    "* 調べたクラスタ数 × 50(GloVeのベクトルサイズ)がSCDVのベクトルサイズとなる。このサイズでElasticsearch用のベクトルインデックスを登録する。\n",
    "* 以下でElasticsearch用のベクトルインデックスを登録するが、es_vector_schema.txx内の\"dense_vector\"の\"dimes\"をこのベクトルサイズに書き換えること。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"acknowledged\":true,\"shards_acknowledged\":true,\"index\":\"vector\"}"
     ]
    }
   ],
   "source": [
    "# ベクターインデックスを登録\n",
    "! curl -XPUT -H \"Content-Type: application/json\" elasticsearch:9200/vector -d @es_vector_schema.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCDVのGMMモデルを構築する。\n",
    "* 引数のnum_clustersは上記で調べたLDAトピックモデル数を参考にして設定する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering Done...\n",
      "Cluster Assignments Saved...\n",
      "Probabilities of Cluster Assignments saved...\n",
      "100%|██████████████████████████████████| 42143/42143 [00:02<00:00, 14187.04it/s]\n",
      "100%|████████████████████████████████████| 4683/4683 [00:00<00:00, 22601.14it/s]\n",
      "train size:42143  vector size:200\n",
      "test size:4683  vector size:200\n",
      "Test start...\n",
      "Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cause   0.431548  0.331050  0.374677       438\n",
      "    measures   0.619748  0.590000  0.604508       500\n",
      "   situation   0.505525  0.375770  0.431095       487\n",
      "       title   0.425000  0.195402  0.267717        87\n",
      "      その他の事業   0.476190  0.164835  0.244898       182\n",
      "      保健・衛生業   0.000000  0.000000  0.000000         1\n",
      "        保健衛生   0.000000  0.000000  0.000000         1\n",
      "       保健衛生業   0.056604  0.187500  0.086957        16\n",
      "          商業   0.440476  0.252560  0.321041       293\n",
      "         官公署   0.000000  0.000000  0.000000         1\n",
      "         建設業   0.582888  0.730930  0.648568      1193\n",
      "       接客娯楽業   0.100000  0.041667  0.058824        48\n",
      "      教育・研究業   0.034483  0.500000  0.064516         2\n",
      "       教育研究業   0.024390  0.090909  0.038462        11\n",
      "      映画・演劇業   0.000000  0.000000  0.000000         3\n",
      "      清掃・と畜業   0.515152  0.160377  0.244604       106\n",
      "      畜産・水産業   0.237288  0.368421  0.288660        38\n",
      "         製造業   0.463221  0.416071  0.438382       560\n",
      "       貨物取扱業   0.066667  0.046512  0.054795        43\n",
      "         農林業   0.621849  0.544118  0.580392       136\n",
      "         通信業   0.034483  0.181818  0.057971        11\n",
      "       運輸交通業   0.536496  0.634989  0.581602       463\n",
      "      金融・広告業   0.040000  0.086957  0.054795        23\n",
      "          鉱業   0.125000  0.150000  0.136364        40\n",
      "\n",
      "    accuracy                       0.484091      4683\n",
      "   macro avg   0.264042  0.252079  0.232451      4683\n",
      "weighted avg   0.504884  0.484091  0.481968      4683\n",
      "\n",
      "Accuracy:  0.48409139440529575\n",
      "Time taken: 27.619330883026123 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python scdv.py --csv_file ./tokenized/merge_tokenized.csv --num_clusters 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全データについてSCDVベクトルを算出し、Elasticsearchに登録する\n",
    "!python scdv_to_es.py --host elasticsearch --input_csv ./tokenized/merge_tokenized.csv --num_clusters 4 > ./log/scdv_to_es.log 2>&1 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
