{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. データをクローリングする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p html excel csv tokenized vector log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 事故事例(Excel系)と、事例・状況・原因・対策（HTML系）の2種類を取得する。\n",
    "! ./get_doc.sh > ./log/get_doc.log 2>&1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 取得したデータをCSVに変換する。\n",
    "* 事故事例のExcelと、事例・状況・原因・対策のHTMLをcsvに変換する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excel to CSV\n",
    "!python excel_to_csv.py ./excel/ ./csv/ > ./log/excel_to_csv.log 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML to CSV\n",
    "!python html_to_csv.py ./html/ ./csv/anzen_all.csv > ./log/html_to_csv.log 2>&1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ElasticsearchにIndexを登録する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"acknowledged\":true,\"shards_acknowledged\":true,\"index\":\"anzen\"}"
     ]
    }
   ],
   "source": [
    "# 事例・状況・原因・対策用のインデックスを登録\n",
    "! curl -XPUT -H \"Content-Type: application/json\" elasticsearch:9200/anzen -d @es_anzen_csv_schema.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"acknowledged\":true,\"shards_acknowledged\":true,\"index\":\"accident\"}"
     ]
    }
   ],
   "source": [
    "# 事例用のインデックスを登録\n",
    "! curl -XPUT -H \"Content-Type: application/json\" elasticsearch:9200/accident -d @es_accident_schema.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Elasticsearchにデータをロードする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 事例・状況・原因・対策データをロード(anzenインデックス)\n",
    "! python load_anzen_es.py --host elasticsearch --index anzen --input_csv ./csv/anzen_all.csv  > ./log/load_anzen_es.log 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 事例用データをロード(accidentインデックス)\n",
    "!python load_accident_es.py --host elasticsearch --index accident --input_dir ./csv/  > ./log/load_accident_es.log 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Elasticsearchでトークナイズし、結果をCSV形式とテキスト形式で保存する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 事例・状況・原因・対策データの文章をトークナイズし、CSVとTXTファイルとして保存する\n",
    "!python es_anzen_tokenize.py --host elasticsearch --index anzen --output tokenized/anzen_tokenized > ./log/es_anzen_tokenize.log 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 事例用データの文章をトークナイズし、CSVとTXTファイルとして保存する\n",
    "!python es_accident_tokenize.py --host elasticsearch --index accident --output tokenized/accident_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. GloVeで単語を学習する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'GloVe' already exists and is not an empty directory.\n",
      "mkdir -p build\n"
     ]
    }
   ],
   "source": [
    "# GloVeを取得する。最初に1回だけ実行すれば良い。\n",
    "! git clone https://github.com/stanfordnlp/GloVe.git\n",
    "! cd GloVe && make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トークナイズしたanzenとaccidentのテキストファイルを結合する。\n",
    "! cat tokenized/anzen_tokenized.txt tokenized/accident_tokenized.txt > tokenized/all_tokens.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "$ GloVe/build/vocab_count -min-count 0 -verbose 2 < tokenized/all_tokens.txt > ./vector/glove_vocab.txt\n",
      "BUILDING VOCABULARY\n",
      "Processed 0 tokens.\u001b[11G100000 tokens.\u001b[11G200000 tokens.\u001b[11G300000 tokens.\u001b[11G400000 tokens.\u001b[11G500000 tokens.\u001b[11G600000 tokens.\u001b[11G700000 tokens.\u001b[11G800000 tokens.\u001b[11G900000 tokens.\u001b[11G1000000 tokens.\u001b[11G1100000 tokens.\u001b[0GProcessed 1120989 tokens.\n",
      "Counted 22222 unique words.\n",
      "Using vocabulary of size 22222.\n",
      "\n",
      "$ GloVe/build/cooccur -memory 4.0 -vocab-file ./vector/glove_vocab.txt -verbose 2 -window-size 15 < tokenized/all_tokens.txt > ./vector/glove_cooccurrence.bin\n",
      "COUNTING COOCCURRENCES\n",
      "window size: 15\n",
      "context: symmetric\n",
      "max product: 13752509\n",
      "overflow length: 38028356\n",
      "Reading vocab from file \"./vector/glove_vocab.txt\"...loaded 22222 words.\n",
      "Building lookup table...table contains 62977881 elements.\n",
      "Processing token: 0\u001b[19G100000\u001b[19G200000\u001b[19G300000\u001b[19G400000\u001b[19G500000\u001b[19G600000\u001b[19G700000\u001b[19G800000\u001b[19G900000\u001b[19G1000000\u001b[19G1100000\u001b[0GProcessed 1120989 tokens.\n",
      "Writing cooccurrences to disk........2 files in total.\n",
      "Merging cooccurrence files: processed 0 lines.\u001b[39G100000 lines.\u001b[39G200000 lines.\u001b[39G300000 lines.\u001b[39G400000 lines.\u001b[39G500000 lines.\u001b[39G600000 lines.\u001b[39G700000 lines.\u001b[39G800000 lines.\u001b[39G900000 lines.\u001b[39G1000000 lines.\u001b[39G1100000 lines.\u001b[39G1200000 lines.\u001b[39G1300000 lines.\u001b[39G1400000 lines.\u001b[39G1500000 lines.\u001b[39G1600000 lines.\u001b[39G1700000 lines.\u001b[39G1800000 lines.\u001b[39G1900000 lines.\u001b[39G2000000 lines.\u001b[39G2100000 lines.\u001b[39G2200000 lines.\u001b[39G2300000 lines.\u001b[39G2400000 lines.\u001b[39G2500000 lines.\u001b[39G2600000 lines.\u001b[39G2700000 lines.\u001b[39G2800000 lines.\u001b[39G2900000 lines.\u001b[39G3000000 lines.\u001b[39G3100000 lines.\u001b[39G3200000 lines.\u001b[39G3300000 lines.\u001b[39G3400000 lines.\u001b[39G3500000 lines.\u001b[39G3600000 lines.\u001b[39G3700000 lines.\u001b[39G3800000 lines.\u001b[39G3900000 lines.\u001b[39G4000000 lines.\u001b[39G4100000 lines.\u001b[39G4200000 lines.\u001b[39G4300000 lines.\u001b[39G4400000 lines.\u001b[39G4500000 lines.\u001b[39G4600000 lines.\u001b[39G4700000 lines.\u001b[39G4800000 lines.\u001b[0GMerging cooccurrence files: processed 4849940 lines.\n",
      "\n",
      "$ GloVe/build/shuffle -memory 4.0 -verbose 2 < ./vector/glove_cooccurrence.bin > ./vector/glove_cooccurrence_shuf.bin\n",
      "Using random seed 1581836711\n",
      "SHUFFLING COOCCURRENCES\n",
      "array size: 255013683\n",
      "Shuffling by chunks: processed 0 lines.\u001b[22Gprocessed 4849940 lines.\n",
      "Wrote 1 temporary file(s).\n",
      "Merging temp files: processed 0 lines.\u001b[31G4849940 lines.\u001b[0GMerging temp files: processed 4849940 lines.\n",
      "\n",
      "$ GloVe/build/glove -save-file ./vector/glove_vectors -threads 8 -input-file ./vector/glove_cooccurrence_shuf.bin -x-max 10 -iter 50 -vector-size 50 -binary 2 -vocab-file ./vector/glove_vocab.txt -verbose 2\n",
      "TRAINING MODEL\n",
      "Read 4849940 lines.\n",
      "Initializing parameters...Using random seed 1581836712\n",
      "done.\n",
      "vector size: 50\n",
      "vocab size: 22222\n",
      "x_max: 10.000000\n",
      "alpha: 0.750000\n",
      "02/16/20 - 04:05.12PM, iter: 001, cost: 0.111791\n",
      "02/16/20 - 04:05.13PM, iter: 002, cost: 0.079834\n",
      "02/16/20 - 04:05.13PM, iter: 003, cost: 0.066692\n",
      "02/16/20 - 04:05.14PM, iter: 004, cost: 0.057942\n",
      "02/16/20 - 04:05.14PM, iter: 005, cost: 0.051310\n",
      "02/16/20 - 04:05.15PM, iter: 006, cost: 0.046405\n",
      "02/16/20 - 04:05.15PM, iter: 007, cost: 0.042889\n",
      "02/16/20 - 04:05.16PM, iter: 008, cost: 0.040338\n",
      "02/16/20 - 04:05.16PM, iter: 009, cost: 0.038420\n",
      "02/16/20 - 04:05.17PM, iter: 010, cost: 0.036971\n",
      "02/16/20 - 04:05.17PM, iter: 011, cost: 0.035815\n",
      "02/16/20 - 04:05.18PM, iter: 012, cost: 0.034892\n",
      "02/16/20 - 04:05.18PM, iter: 013, cost: 0.034103\n",
      "02/16/20 - 04:05.18PM, iter: 014, cost: 0.033462\n",
      "02/16/20 - 04:05.19PM, iter: 015, cost: 0.032906\n",
      "02/16/20 - 04:05.19PM, iter: 016, cost: 0.032432\n",
      "02/16/20 - 04:05.20PM, iter: 017, cost: 0.032019\n",
      "02/16/20 - 04:05.20PM, iter: 018, cost: 0.031633\n",
      "02/16/20 - 04:05.21PM, iter: 019, cost: 0.031304\n",
      "02/16/20 - 04:05.21PM, iter: 020, cost: 0.031001\n",
      "02/16/20 - 04:05.22PM, iter: 021, cost: 0.030736\n",
      "02/16/20 - 04:05.22PM, iter: 022, cost: 0.030490\n",
      "02/16/20 - 04:05.23PM, iter: 023, cost: 0.030263\n",
      "02/16/20 - 04:05.23PM, iter: 024, cost: 0.030058\n",
      "02/16/20 - 04:05.24PM, iter: 025, cost: 0.029868\n",
      "02/16/20 - 04:05.24PM, iter: 026, cost: 0.029692\n",
      "02/16/20 - 04:05.25PM, iter: 027, cost: 0.029519\n",
      "02/16/20 - 04:05.25PM, iter: 028, cost: 0.029368\n",
      "02/16/20 - 04:05.26PM, iter: 029, cost: 0.029226\n",
      "02/16/20 - 04:05.26PM, iter: 030, cost: 0.029089\n",
      "02/16/20 - 04:05.27PM, iter: 031, cost: 0.028969\n",
      "02/16/20 - 04:05.27PM, iter: 032, cost: 0.028844\n",
      "02/16/20 - 04:05.28PM, iter: 033, cost: 0.028738\n",
      "02/16/20 - 04:05.28PM, iter: 034, cost: 0.028631\n",
      "02/16/20 - 04:05.29PM, iter: 035, cost: 0.028526\n",
      "02/16/20 - 04:05.29PM, iter: 036, cost: 0.028433\n",
      "02/16/20 - 04:05.30PM, iter: 037, cost: 0.028344\n",
      "02/16/20 - 04:05.30PM, iter: 038, cost: 0.028257\n",
      "02/16/20 - 04:05.31PM, iter: 039, cost: 0.028175\n",
      "02/16/20 - 04:05.31PM, iter: 040, cost: 0.028096\n",
      "02/16/20 - 04:05.31PM, iter: 041, cost: 0.028022\n",
      "02/16/20 - 04:05.32PM, iter: 042, cost: 0.027940\n",
      "02/16/20 - 04:05.32PM, iter: 043, cost: 0.027872\n",
      "02/16/20 - 04:05.33PM, iter: 044, cost: 0.027806\n",
      "02/16/20 - 04:05.33PM, iter: 045, cost: 0.027743\n",
      "02/16/20 - 04:05.34PM, iter: 046, cost: 0.027680\n",
      "02/16/20 - 04:05.34PM, iter: 047, cost: 0.027624\n",
      "02/16/20 - 04:05.35PM, iter: 048, cost: 0.027566\n",
      "02/16/20 - 04:05.35PM, iter: 049, cost: 0.027510\n",
      "02/16/20 - 04:05.36PM, iter: 050, cost: 0.027458\n"
     ]
    }
   ],
   "source": [
    "# GloVeで学習する。\n",
    "! ./glove.sh "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCDVで文章をベクトル化する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anzenとaccidentのCSVをマージする。ベクトル化に必要な列のみ選別し、列名を統一する。\n",
    "!python merge_csv.py --input_anzen tokenized/anzen_tokenized.csv --input_accident tokenized/accident_tokenized.csv --output_csv tokenized/merge_tokenized.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDAトピックモデルを使用して、いくつのトピックに分類すると良いか確認する\n",
    "* 以降のSCDVモデルを構築する際の、GMMのクラスタ数の参考とするため\n",
    "* 調べたクラスタ数 × 50(GloVeのベクトルサイズ)がSCDVのベクトルサイズとなる。このサイズでElasticsearch用のベクトルインデックスを登録する。\n",
    "* 以下でElasticsearch用のベクトルインデックスを登録するが、es_vector_schema.txx内の\"dense_vector\"の\"dimes\"をこのベクトルサイズに書き換えること。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"acknowledged\":true,\"shards_acknowledged\":true,\"index\":\"vector\"}"
     ]
    }
   ],
   "source": [
    "# ベクターインデックスを登録\n",
    "! curl -XPUT -H \"Content-Type: application/json\" elasticsearch:9200/vector -d @es_vector_schema.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCDVのGMMモデルを構築する。\n",
    "* 引数のnum_clustersは上記で調べたLDAトピックモデル数を参考にして設定する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering Done...\n",
      "Cluster Assignments Saved...\n",
      "Probabilities of Cluster Assignments saved...\n",
      "100%|██████████████████████████████████| 42143/42143 [00:02<00:00, 14544.86it/s]\n",
      "100%|████████████████████████████████████| 4683/4683 [00:00<00:00, 23086.18it/s]\n",
      "train size:42143  vector size:200\n",
      "test size:4683  vector size:200\n",
      "Test start...\n",
      "Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cause   0.497126  0.394977  0.440204       438\n",
      "    measures   0.646934  0.612000  0.628983       500\n",
      "   situation   0.546419  0.422998  0.476852       487\n",
      "       title   0.400000  0.160920  0.229508        87\n",
      "      その他の事業   0.358209  0.131868  0.192771       182\n",
      "      保健・衛生業   0.000000  0.000000  0.000000         1\n",
      "        保健衛生   0.000000  0.000000  0.000000         1\n",
      "       保健衛生業   0.000000  0.000000  0.000000        16\n",
      "          商業   0.439252  0.320819  0.370809       293\n",
      "         官公署   0.000000  0.000000  0.000000         1\n",
      "         建設業   0.566540  0.749371  0.645254      1193\n",
      "       接客娯楽業   0.600000  0.062500  0.113208        48\n",
      "      教育・研究業   0.000000  0.000000  0.000000         2\n",
      "       教育研究業   0.046512  0.181818  0.074074        11\n",
      "      映画・演劇業   0.041667  0.333333  0.074074         3\n",
      "      清掃・と畜業   0.481481  0.122642  0.195489       106\n",
      "      畜産・水産業   0.333333  0.315789  0.324324        38\n",
      "         製造業   0.452830  0.428571  0.440367       560\n",
      "       貨物取扱業   0.114286  0.093023  0.102564        43\n",
      "         農林業   0.526316  0.514706  0.520446       136\n",
      "         通信業   0.017241  0.090909  0.028986        11\n",
      "       運輸交通業   0.541841  0.559395  0.550478       463\n",
      "      金融・広告業   0.075000  0.130435  0.095238        23\n",
      "          鉱業   0.195652  0.225000  0.209302        40\n",
      "\n",
      "    accuracy                       0.497117      4683\n",
      "   macro avg   0.286693  0.243795  0.238039      4683\n",
      "weighted avg   0.511579  0.497117  0.491155      4683\n",
      "\n",
      "Accuracy:  0.4971172325432415\n",
      "Time taken: 20.060561418533325 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python scdv.py --csv_file ./tokenized/merge_tokenized.csv --num_clusters 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全データについてSCDVベクトルを算出し、Elasticsearchに登録する\n",
    "!python scdv_to_es.py --host elasticsearch --input_csv ./tokenized/merge_tokenized.csv --num_clusters 4 > ./log/scdv_to_es.log 2>&1 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
